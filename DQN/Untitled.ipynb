{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "input_shape = [4]\n",
    "n_outputs = 2\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation='elu', input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation='elu'),\n",
    "    keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.randn() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_value = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_value[0])\n",
    "# replay_buffer 存储的是 <= maxlen 个五元组\n",
    "replay_buffer = deque(maxlen=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_experiences(batch_size):\n",
    "    # 从 replay_buffer 中取出 batch_size 个元素\n",
    "    indic_ind = np.random.randint(len(replay_buffer), size=batch_size )\n",
    "    experiences = [replay_buffer[experience] for experience in indic_ind]\n",
    "    states, actions, rewards, nexts, dones = [\n",
    "        np.array([experience[index] for experience in experiences])\n",
    "        for index in range(5)\n",
    "    ]\n",
    "    return states, actions, rewards, nexts, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    nextstate, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, nextstate, done))\n",
    "\n",
    "    return nextstate, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_step(batch_size):\n",
    "    # Q_learning Target: reward + gamma*argmax{a'}Q(s', a') - Q(s, a)\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, nexts, dones = experiences\n",
    "    # 计算Q(s',a')时不能放在 tape 中，因为这个时候需要 fixed network\n",
    "    Q_values = np.max(model.predict(nexts), axis=1)\n",
    "    Q_Target = rewards + discount_factor * Q_values\n",
    "    # 使用 one-hot 编码 & 元素乘法 再根据坐标轴求和 结果会变成Q_values结构\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Wrong\n",
    "        # Q_values = tf.reduce_sum(mask * model.predict(states), axis=1, keepdims=True)\n",
    "        # 因为是在训练阶段，如果使用predict函数，所进行的操作不会把变量算成trainable_variables\n",
    "        Q_values = tf.reduce_sum(mask * model(states), axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(Q_Target, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "rurn_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.randn(2, 32, 32, 3)\n",
    "        tf.summary.image(\"my_image\", images * step / 1000, step=step)\n",
    "        texts = ['the step is ' + str(step) , \"its ssquare is \" + str(step**2)]\n",
    "        tf.summary.text('my_text', texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
